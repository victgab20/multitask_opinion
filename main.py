path = r"C:\Users\victo\Downloads\corpus\labeled"
# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cma9e9Ctbb6lptSWbP1GMR1u8iSQkrQH
"""



import pandas as pd
import torch
from torch.utils.data import DataLoader, Dataset
from src.tensorboard.polarity.board import TensorBoardLogger 
import torch.nn as nn
import torch.optim as optim
import time
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, f1_score
import pandas as pd
import numpy as np
import re
import string
from sklearn.feature_extraction.text import TfidfVectorizer
import pickle
from transformers import BertTokenizer
from transformers import BertModel

logger = TensorBoardLogger(log_dir='runs/experiment')

dataset_test_apps = pd.read_pickle(f"{path}/test_apps.pkl")
dataset_train_apps = pd.read_pickle(f"{path}/train_apps.pkl")
dataset_dev_apps= pd.read_pickle(f"{path}/dev_apps.pkl")

dataset_test = pd.read_pickle(f"{path}/test_filmes.pkl")
dataset_train = pd.read_pickle(f"{path}/train_filmes.pkl")
dataset_dev = pd.read_pickle(f"{path}/dev_filmes.pkl")

dataset_test_apps.info()

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def preprocess_text(text: str):
    text = text.lower()
    text = re.sub(r'[0-9]+', '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def pre_processing(df: pd.DataFrame):
    df = df.loc[lambda x: x["stars"] != 3]
    df["stars"] = df["stars"].apply(lambda x: 1 if x > 3 else 0)
    df["text"] = df["text"].apply(preprocess_text)
    df = df[df['text'].notnull() & (df['text'] != '')]
    return df

class MultiTaskBertModel(nn.Module):
    def __init__(self, model_name="bert-base-uncased", hidden_size=768):
        super(MultiTaskBertModel, self).__init__()
        self.bert = BertModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.3)
        self.fc_polarity = nn.Linear(hidden_size, 1)  # Para a tarefa de polaridade
        self.fc_usefulness = nn.Linear(hidden_size, 1)  # Para a tarefa de utilidade

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output  # Pega a saída [CLS] token

        pooled_output = self.dropout(pooled_output)

        # Saída para a tarefa de polaridade
        polarity_output = self.fc_polarity(pooled_output)

        # Saída para a tarefa de utilidade
        usefulness_output = self.fc_usefulness(pooled_output)

        return polarity_output, usefulness_output

from torch import nn

class MultiTaskLoss(nn.Module):
    def __init__(self):
        super(MultiTaskLoss, self).__init__()
        self.loss_fn_polarity = nn.BCEWithLogitsLoss()  # Loss para polaridade
        self.loss_fn_usefulness = nn.BCEWithLogitsLoss()  # Loss para utilidade

    def forward(self, polarity_output, usefulness_output, polarity_labels, usefulness_labels):
        loss_polarity = self.loss_fn_polarity(polarity_output.squeeze(), polarity_labels)
        loss_usefulness = self.loss_fn_usefulness(usefulness_output.squeeze(), usefulness_labels)

        total_loss = loss_polarity + loss_usefulness
        return total_loss

class ReviewDataset(Dataset):
    def __init__(self, df, tokenizer, max_length=128):
        self.texts = df["text"].tolist()
        self.stars = df["stars"].tolist()
        self.usefulness = df["usefulness"].tolist()  # Adicionando a coluna de utilidade
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        encoding = self.tokenizer(
            self.texts[idx],
            max_length=self.max_length,
            padding="max_length",
            truncation=True
        )

        return {
            "input_ids": torch.tensor(encoding["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(encoding["attention_mask"], dtype=torch.long),
            "stars": torch.tensor(self.stars[idx], dtype=torch.float),
            "usefulness": torch.tensor(self.usefulness[idx], dtype=torch.float)
        }

train_data = pre_processing(dataset_train_apps[["stars", "text", "helpfulness"]])
dev_data = pre_processing(dataset_dev_apps[["stars", "text", "helpfulness"]])
test_data = pre_processing(dataset_test_apps[["stars", "text", "helpfulness"]])

train_data['usefulness'] = (train_data['helpfulness'] > 0.5).astype(int)
dev_data['usefulness'] = (dev_data['helpfulness'] > 0.5).astype(int)
test_data['usefulness'] = (test_data['helpfulness'] > 0.5).astype(int)

train_dataset = ReviewDataset(train_data, tokenizer)
dev_dataset = ReviewDataset(dev_data, tokenizer)
test_dataset = ReviewDataset(test_data, tokenizer)

BATCH_SIZE = 16

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score
from torch.optim import AdamW
from tqdm import tqdm

# Instanciar o modelo e a função de perda
model = MultiTaskBertModel()
criterion = MultiTaskLoss()
optimizer = AdamW(model.parameters(), lr=2e-5)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Função para calcular as métricas
def compute_metrics(true_labels, predictions):
    accuracy = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions, average='weighted')
    recall = recall_score(true_labels, predictions, average='weighted')
    precision = precision_score(true_labels, predictions, average='weighted')
    return accuracy, f1, recall, precision

# Treinamento
def train_model(model, train_loader, optimizer, criterion, epoch):
    model.train()
    total_loss = 0
    all_polarity_preds, all_usefulness_preds = [], []
    all_polarity_labels, all_usefulness_labels = [], []

    for batch in tqdm(train_loader):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        polarity_labels = batch["stars"].to(device)
        usefulness_labels = batch["usefulness"].to(device)

        optimizer.zero_grad()

        # Forward pass
        polarity_output, usefulness_output = model(input_ids, attention_mask)

        # Calcular a perda
        loss = criterion(polarity_output, usefulness_output, polarity_labels, usefulness_labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        # Armazenar as previsões e labels para calcular métricas
        all_polarity_preds.extend(torch.sigmoid(polarity_output).cpu().detach().numpy() > 0.5)
        all_usefulness_preds.extend(torch.sigmoid(usefulness_output).cpu().detach().numpy() > 0.5)
        all_polarity_labels.extend(polarity_labels.cpu().detach().numpy())
        all_usefulness_labels.extend(usefulness_labels.cpu().detach().numpy())

    # Calcular métricas
    train_polarity_metrics = compute_metrics(all_polarity_labels, all_polarity_preds)
    train_usefulness_metrics = compute_metrics(all_usefulness_labels, all_usefulness_preds)

    # Logar métricas no TensorBoard
    logger.log_metrics(
        epoch, 
        total_loss / len(train_loader), train_polarity_metrics[0], 0, 0, 
        train_polarity_metrics[3], train_polarity_metrics[2], train_polarity_metrics[1],
        train_usefulness_metrics[3], train_usefulness_metrics[2], train_usefulness_metrics[1]
    )

    # Imprimir métricas
    print(f"Polarity - Accuracy: {train_polarity_metrics[0]:.4f}, F1: {train_polarity_metrics[1]:.4f}, Recall: {train_polarity_metrics[2]:.4f}, Precision: {train_polarity_metrics[3]:.4f}")
    print(f"Usefulness - Accuracy: {train_usefulness_metrics[0]:.4f}, F1: {train_usefulness_metrics[1]:.4f}, Recall: {train_usefulness_metrics[2]:.4f}, Precision: {train_usefulness_metrics[3]:.4f}")

    return total_loss / len(train_loader)


# Função de avaliação
def evaluate_model(model, test_loader, epoch):
    model.eval()
    all_polarity_preds, all_usefulness_preds = [], []
    all_polarity_labels, all_usefulness_labels = [], []
    total_loss = 0

    with torch.no_grad():
        for batch in tqdm(test_loader):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            polarity_labels = batch["stars"].to(device)
            usefulness_labels = batch["usefulness"].to(device)

            polarity_output, usefulness_output = model(input_ids, attention_mask)

            all_polarity_preds.extend(torch.sigmoid(polarity_output).cpu().detach().numpy() > 0.5)
            all_usefulness_preds.extend(torch.sigmoid(usefulness_output).cpu().detach().numpy() > 0.5)
            all_polarity_labels.extend(polarity_labels.cpu().detach().numpy())
            all_usefulness_labels.extend(usefulness_labels.cpu().detach().numpy())

    # Calcular métricas
    dev_polarity_metrics = compute_metrics(all_polarity_labels, all_polarity_preds)
    dev_usefulness_metrics = compute_metrics(all_usefulness_labels, all_usefulness_preds)

    # Logar métricas no TensorBoard
    logger.log_metrics(
        epoch, 
        0, 0, total_loss / len(test_loader), dev_polarity_metrics[0], 
        dev_polarity_metrics[3], dev_polarity_metrics[2], dev_polarity_metrics[1],
        dev_usefulness_metrics[3], dev_usefulness_metrics[2], dev_usefulness_metrics[1]
    )

    # Imprimir métricas
    print("\n--- Test Set Metrics ---")
    print(f"Polarity - Accuracy: {dev_polarity_metrics[0]:.4f}, F1: {dev_polarity_metrics[1]:.4f}, Recall: {dev_polarity_metrics[2]:.4f}, Precision: {dev_polarity_metrics[3]:.4f}")
    print(f"Usefulness - Accuracy: {dev_usefulness_metrics[0]:.4f}, F1: {dev_usefulness_metrics[1]:.4f}, Recall: {dev_usefulness_metrics[2]:.4f}, Precision: {dev_usefulness_metrics[3]:.4f}")


# Definir número de épocas
num_epochs = 5  
model = model.to(device)

# Loop de treinamento e avaliação
for epoch in range(num_epochs):
    print(f"Epoch {epoch + 1}/{num_epochs}")
    
    # Treinar o modelo
    avg_train_loss = train_model(model, train_loader, optimizer, criterion, epoch)
    print(f"Average Train Loss: {avg_train_loss:.4f}\n")

    # Avaliar no conjunto de teste
    evaluate_model(model, test_loader, epoch)

# Fechar o logger após o treinamento
logger.close()